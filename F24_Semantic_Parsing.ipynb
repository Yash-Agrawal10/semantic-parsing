{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b0MLqDYLdLHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1772859-9a40-4ca0-e547-d1081bb5412d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"/content/drive/MyDrive/CSCI 1460/Final Project/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hXiL6mlFmssL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3e20f0-561d-4ee5-ad37-c763154795d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\n",
        "response = requests.get('http://dong.li/lang2logic/seq2seq_jobqueries.zip')\n",
        "if response.status_code == 200:\n",
        "  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
        "  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(FILEPATH)\n",
        "  print(\"Extraction completed.\")\n",
        "else:\n",
        "  print(\"Failed to download the zip file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "  \"\"\"Defines a Dataset object for the Jobs dataset to be used with Dataloader\"\"\"\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list[tuple[list[int], list[int]]]\n",
        "      a list of outputs of __getitem__\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a batched set of input sequences and a batched set of target sequences\n",
        "  \"\"\"\n",
        "  src, tgt = default_collate(batch)\n",
        "  return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# TODO: Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               src_embed_size: int,\n",
        "               src_vocab_size: int,\n",
        "               hidden_size: int,\n",
        "               num_layers: int,\n",
        "               dropout: float):\n",
        "    \"\"\"\n",
        "    Initializes an Encoder module that uses an LSTM\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      src_embed_size: int\n",
        "        size of embeddings for source vocab\n",
        "\n",
        "      src_vocab_size: int\n",
        "        vocab size of source\n",
        "\n",
        "      hidden_size: int\n",
        "        size of hidden layer in LSTM\n",
        "\n",
        "      num_layers: int\n",
        "        number of layers in LSTM\n",
        "\n",
        "      dropout : float\n",
        "        dropout used for LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(src_vocab_size, src_embed_size)\n",
        "    self.encoder = nn.LSTM(src_embed_size, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Full encoder forward pass\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      x: torch.Tensor\n",
        "        Vector of indexes representing input sequence\n",
        "\n",
        "      Returns\n",
        "      ----------\n",
        "      Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
        "        torch.Tensor - output of LSTM (encodings)\n",
        "        Tuple[torch.Tensor, torch.Tensor] - last hidden cell states of LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = self.embedding(x)\n",
        "    output, (h_n, c_n) = self.encoder(embeddings)\n",
        "    return output, (h_n, c_n)"
      ],
      "metadata": {
        "id": "qeoLGvIZAIIJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size: int, dropout: float):\n",
        "    \"\"\"\n",
        "    Initializes an attention module\n",
        "\n",
        "    Paramters\n",
        "    ---------\n",
        "    hidden_size: int\n",
        "      size of hidden state in encoder AND decoder\n",
        "\n",
        "    dropout: float\n",
        "      dropout used for linear layers\n",
        "    \"\"\"\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, encodings: torch.Tensor, hidden_state: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Applies attention to decoder output using encodings\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encodings: torch.Tensor\n",
        "      Full encodings output by encoder\n",
        "\n",
        "    hidden_state: torch.Tensor\n",
        "      Final hidden state output by decoder\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "      Hidden state with attention applied\n",
        "    \"\"\"\n",
        "\n",
        "    weights = torch.bmm(encodings, hidden_state.unsqueeze(2)).squeeze(2)\n",
        "    weights = torch.softmax(weights, dim=1)\n",
        "    context = torch.bmm(weights.unsqueeze(1), encodings).squeeze(1)\n",
        "    h_att = self.activation(self.W1(hidden_state) + self.W2(context))\n",
        "    h_att = self.dropout(h_att)\n",
        "    return h_att\n"
      ],
      "metadata": {
        "id": "1v5vwC0mOgvb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               tgt_embed_size: int,\n",
        "               tgt_vocab_size: int,\n",
        "               hidden_size: int,\n",
        "               num_layers: int,\n",
        "               dropout: float,\n",
        "               attention: Attention):\n",
        "    \"\"\"\n",
        "    Initializes a Decoder module that uses an LSTM\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      tgt_emb_size: int\n",
        "        size of embeddings for target vocab\n",
        "\n",
        "      tgt_vocab_size: int\n",
        "        vocab size of target\n",
        "\n",
        "      hidden_size: int\n",
        "        size of hidden layer in LSTM\n",
        "\n",
        "      num_layers: int\n",
        "        number of layers in LSTM\n",
        "\n",
        "      dropout : float\n",
        "        dropout used in LSTM and output layer\n",
        "\n",
        "      attention: Attention\n",
        "        attention module to be used in decoder\n",
        "    \"\"\"\n",
        "\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(tgt_vocab_size, tgt_embed_size)\n",
        "    self.decoder = nn.LSTM(tgt_embed_size, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "    self.attention = attention\n",
        "    self.output = nn.Linear(hidden_size, tgt_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,\n",
        "              encodings: torch.Tensor,\n",
        "              prev_token: torch.Tensor,\n",
        "              state: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Single decoder pass to return next token in sequence\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      encodings: torch.Tensor\n",
        "        Encodings generated by encoder from original input sequence\n",
        "\n",
        "      prev_token: torch.Tensor\n",
        "        Index of previously predicted token\n",
        "\n",
        "      state: Tuple[torch.Tensor, torch.Tensor]\n",
        "        tuple of (hidden_state, cell_state) to pass to LSTM\n",
        "        uses previous state of decoder, or final state of encoder for first pass\n",
        "\n",
        "      Returns\n",
        "      ----------\n",
        "      Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
        "        torch.Tensor - output of LSTM (raw logits)\n",
        "        Tuple[torch.Tensor, torch.Tensor] - last hidden and cell states of LSTM\n",
        "    \"\"\"\n",
        "    prev_embedding = self.embedding(prev_token).unsqueeze(1)\n",
        "    decodings, (h_n, c_n) = self.decoder(prev_embedding, state)\n",
        "    final_hidden = h_n[-1]\n",
        "\n",
        "    h_att = self.attention(encodings, final_hidden)\n",
        "\n",
        "    output = self.output(h_att)\n",
        "    output = self.dropout(output)\n",
        "    return output, (h_n, c_n)"
      ],
      "metadata": {
        "id": "H-bkwIFBDhuz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import random\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 src_embed_size: int,\n",
        "                 tgt_embed_size: int,\n",
        "                 hidden_size: int,\n",
        "                 num_layers: int,\n",
        "                 dropout: float):\n",
        "        \"\"\"\n",
        "        Initializes a Seq2Seq model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        src_vocab_size: int\n",
        "          vocab size of source\n",
        "\n",
        "        tgt_vocab_size: int\n",
        "          vocab size of target\n",
        "\n",
        "        src_embed_size: int\n",
        "          size of embeddings for source vocab\n",
        "\n",
        "        tgt_embed_size: int\n",
        "          size of embeddings for target vocab\n",
        "\n",
        "        hidden_size: int\n",
        "          size of hidden layer for BOTH encoder and decoder LSTMs\n",
        "\n",
        "        num_layers: int\n",
        "          number of layers in both encoder and decoder LSTMs\n",
        "\n",
        "        dropout: float\n",
        "          dropout used in all layers\n",
        "        \"\"\"\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_embed_size,\n",
        "            src_vocab_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(hidden_size, dropout)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            tgt_embed_size,\n",
        "            tgt_vocab_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout,\n",
        "            self.attention\n",
        "        )\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, teacher_forcing_ratio: float) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full Seq2Seq forward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        src : torch.Tensor\n",
        "          source sequence tensor\n",
        "\n",
        "        tgt : torch.Tensor\n",
        "          target sequence tensor (for teacher forcing)\n",
        "\n",
        "        teacher_forcing_ratio : float\n",
        "          determines the ratio at which model uses teacher forcing\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            logits for the predicted target sequence\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "        max_len = tgt.size(1)\n",
        "        outputs = torch.zeros(batch_size, max_len, self.decoder.embedding.num_embeddings)\n",
        "\n",
        "        encodings, prev_state = self.encoder(src)\n",
        "        prev_token = tgt[:, 0]\n",
        "\n",
        "        for i in range(1, max_len):\n",
        "          output, prev_state = self.decoder(encodings, prev_token, prev_state)\n",
        "          outputs[:, i, :] = output\n",
        "          if random() < teacher_forcing_ratio:\n",
        "            prev_token = tgt[:, i]\n",
        "          else:\n",
        "            prev_token = output.argmax(dim=1)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6lqcgABUc-xT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NG376y1VUkOh"
      },
      "outputs": [],
      "source": [
        "QUERY_VOCAB_LEN = len(query_vocab)\n",
        "LF_VOCAB_LEN = len(lf_vocab)\n",
        "\n",
        "def create_model():\n",
        "  \"\"\"\n",
        "  Returns your model!\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Seq2Seq\n",
        "      your model!\n",
        "  \"\"\"\n",
        "  src_vocab_size, tgt_vocab_size = QUERY_VOCAB_LEN, LF_VOCAB_LEN\n",
        "  src_embed_size, tgt_embed_size = 150, 150\n",
        "  hidden_size, num_layers = 150, 1\n",
        "  dropout = 0.3\n",
        "  model = Seq2Seq(src_vocab_size,\n",
        "                  tgt_vocab_size,\n",
        "                  src_embed_size,\n",
        "                  tgt_embed_size,\n",
        "                  hidden_size,\n",
        "                  num_layers,\n",
        "                  dropout)\n",
        "\n",
        "  for param in model.parameters():\n",
        "    nn.init.uniform_(param, a=-0.08, b=0.08)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# TODO: Training and testing loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UT5eiZM0AnTf"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int=5,\n",
        "          device: str=\"cuda\") -> nn.Module:\n",
        "  \"\"\"\n",
        "  Trains your model!\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : nn.Module\n",
        "      your model!\n",
        "  train_dataloader : DataLoader\n",
        "      a dataloader of the training data from build_dataloaders\n",
        "  num_epochs : int\n",
        "      number of epochs to train for\n",
        "  device : str\n",
        "      device that the model is running on\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Seq2Seq\n",
        "      your trained model\n",
        "  \"\"\"\n",
        "\n",
        "  # Move model to device and set to train mode\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  # Initialize loss, optimizer, and parameters\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.95)\n",
        "  teacher_forcing = 1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in train_dataloader:\n",
        "      # Prepare src and tgt\n",
        "      src, tgt = src.to(device), tgt.to(device)\n",
        "      src, tgt = src.transpose(0, 1), tgt.transpose(0, 1)\n",
        "\n",
        "      # Get logits, apply log_softmax to them\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(src, tgt, teacher_forcing_ratio=teacher_forcing)\n",
        "      logits = nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "      # Flatten logits and target for loss function\n",
        "      flattened_logits = logits[:, 1:].reshape(-1, logits.size(-1))\n",
        "      flattened_tgt = tgt[:, 1:].reshape(-1)\n",
        "      flattened_logits = flattened_logits.to(device)\n",
        "      flattened_tgt = flattened_tgt.to(device)\n",
        "\n",
        "      # Get clipped loss, gradients, and take optimizer step\n",
        "      loss = loss_fn(flattened_logits, flattened_tgt)\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str=\"cuda\") -> tuple[int, int]:\n",
        "  \"\"\"\n",
        "  Evaluates your model!\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : nn.Module\n",
        "      your model!\n",
        "  dataloader : DataLoader\n",
        "      a dataloader of the testing data from build_dataloaders\n",
        "  device : str\n",
        "      device that the model is running on\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[int, int]\n",
        "      per-token accuracy and exact_match accuracy\n",
        "  \"\"\"\n",
        "\n",
        "  # Move model to device and set to eval mode\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counts used for accuracy\n",
        "  total_tokens = 0\n",
        "  correct_tokens = 0\n",
        "  total_seqs = 0\n",
        "  correct_seqs = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for src, tgt in dataloader:\n",
        "      # Prepare src and tgt\n",
        "      src, tgt = src.to(device), tgt.to(device)\n",
        "      src, tgt = src.transpose(0, 1), tgt.transpose(0, 1)\n",
        "\n",
        "      # Get predictions\n",
        "      logits = model(src, tgt, teacher_forcing_ratio=0)\n",
        "      y_hat = logits.argmax(dim=-1)\n",
        "      y_hat = y_hat.to(device)\n",
        "\n",
        "      # For each tgt/pred pair in the batch\n",
        "      for i in range(tgt.size(0)):\n",
        "        # Find the EOS index and trim before SOS and after EOS inclusive\n",
        "        eos_index = torch.nonzero((tgt[i] == LF_EOS_INDEX), as_tuple=True)[0].item()\n",
        "        tgt_trimmed = tgt[i, 1:eos_index + 1]\n",
        "        y_hat_trimmed = y_hat[i, 1:eos_index + 1]\n",
        "\n",
        "        # Update counts used for accuracy\n",
        "        correct_tokens += torch.count_nonzero(tgt_trimmed == y_hat_trimmed).item()\n",
        "        total_tokens += tgt_trimmed.size(0)\n",
        "        if torch.equal(tgt_trimmed, y_hat_trimmed):\n",
        "          correct_seqs += 1\n",
        "        total_seqs += 1\n",
        "\n",
        "  # Compute desired accuracy metrics\n",
        "  per_token_accuracy = correct_tokens / total_tokens\n",
        "  exact_match_accuracy = correct_seqs / total_seqs\n",
        "  return per_token_accuracy, exact_match_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0qSnLCPeiI1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9096a41-cb64-4a55-cfdb-e7fcb1d587de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.6243874835968017\n",
            "Epoch 2/20, Loss: 1.106929476261139\n",
            "Epoch 3/20, Loss: 1.0697244691848755\n",
            "Epoch 4/20, Loss: 1.0314664912223817\n",
            "Epoch 5/20, Loss: 1.0175537633895875\n",
            "Epoch 6/20, Loss: 1.0112549424171449\n",
            "Epoch 7/20, Loss: 0.9823995518684387\n",
            "Epoch 8/20, Loss: 0.9676519823074341\n",
            "Epoch 9/20, Loss: 0.9394924521446228\n",
            "Epoch 10/20, Loss: 0.936479172706604\n",
            "Epoch 11/20, Loss: 0.9266954207420349\n",
            "Epoch 12/20, Loss: 0.9075261092185974\n",
            "Epoch 13/20, Loss: 0.8810160779953002\n",
            "Epoch 14/20, Loss: 0.8789917635917663\n",
            "Epoch 15/20, Loss: 0.8878867888450622\n",
            "Epoch 16/20, Loss: 0.8574706125259399\n",
            "Epoch 17/20, Loss: 0.8630762839317322\n",
            "Epoch 18/20, Loss: 0.8610024857521057\n",
            "Epoch 19/20, Loss: 0.8420480084419251\n",
            "Epoch 20/20, Loss: 0.8542601418495178\n",
            "Test Per-token Accuracy: 0.8815533980582524\n",
            "Test Exact-match Accuracy: 0.7642857142857142\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    jobs_train, jobs_test = build_datasets()\n",
        "    dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "    model = create_model()\n",
        "    model = train(model, dataloader_train, num_epochs=20, device=device)\n",
        "    test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "    print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_hfJFfYRSFBV",
        "RCKjb4HsMKw-"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}